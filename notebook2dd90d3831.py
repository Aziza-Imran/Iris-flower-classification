{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":420,"sourceType":"datasetVersion","datasetId":19}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:42.932136Z\",\"iopub.execute_input\":\"2024-10-13T10:42:42.932552Z\",\"iopub.status.idle\":\"2024-10-13T10:42:45.633283Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:42.932502Z\",\"shell.execute_reply\":\"2024-10-13T10:42:45.631633Z\"}}\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.tree import DecisionTreeClassifier \n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:45.636649Z\",\"iopub.execute_input\":\"2024-10-13T10:42:45.637545Z\",\"iopub.status.idle\":\"2024-10-13T10:42:45.681393Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:45.637469Z\",\"shell.execute_reply\":\"2024-10-13T10:42:45.680310Z\"}}\ndf = pd.read_csv('/kaggle/input/iris/Iris.csv')\ndf.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:45.682786Z\",\"iopub.execute_input\":\"2024-10-13T10:42:45.683207Z\",\"iopub.status.idle\":\"2024-10-13T10:42:45.712075Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:45.683167Z\",\"shell.execute_reply\":\"2024-10-13T10:42:45.710582Z\"}}\ndf.info()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:45.715138Z\",\"iopub.execute_input\":\"2024-10-13T10:42:45.716170Z\",\"iopub.status.idle\":\"2024-10-13T10:42:45.748899Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:45.716100Z\",\"shell.execute_reply\":\"2024-10-13T10:42:45.747665Z\"}}\ndf.describe()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:45.750412Z\",\"iopub.execute_input\":\"2024-10-13T10:42:45.750888Z\",\"iopub.status.idle\":\"2024-10-13T10:42:45.770722Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:45.750817Z\",\"shell.execute_reply\":\"2024-10-13T10:42:45.769466Z\"}}\ndf.isnull()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:45.772119Z\",\"iopub.execute_input\":\"2024-10-13T10:42:45.772713Z\",\"iopub.status.idle\":\"2024-10-13T10:42:45.785320Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:45.772655Z\",\"shell.execute_reply\":\"2024-10-13T10:42:45.783903Z\"}}\ndf.isnull().sum()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:45.787169Z\",\"iopub.execute_input\":\"2024-10-13T10:42:45.787654Z\",\"iopub.status.idle\":\"2024-10-13T10:42:46.285890Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:45.787610Z\",\"shell.execute_reply\":\"2024-10-13T10:42:46.284739Z\"}}\n# For observing the distribution of each feature\n\nfig_size = (10, 6)\nfig = df[df.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='red', label='Setosa')\ndf[df.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\ndf[df.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length vs sepal Width\")\nfig=plt.gcf()\nfig.set_size_inches(fig_size)\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:46.287326Z\",\"iopub.execute_input\":\"2024-10-13T10:42:46.287711Z\",\"iopub.status.idle\":\"2024-10-13T10:42:46.730431Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:46.287670Z\",\"shell.execute_reply\":\"2024-10-13T10:42:46.729185Z\"}}\nfig = df[df.Species=='Iris-setosa'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='red', label='Setosa')\ndf[df.Species=='Iris-versicolor'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='blue', label='versicolor',ax=fig)\ndf[df.Species=='Iris-virginica'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\" Petal Length vs Petal Width\")\nfig=plt.gcf()\nfig.set_size_inches(fig_size)\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:46.732281Z\",\"iopub.execute_input\":\"2024-10-13T10:42:46.732773Z\",\"iopub.status.idle\":\"2024-10-13T10:42:50.618042Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:46.732717Z\",\"shell.execute_reply\":\"2024-10-13T10:42:50.616777Z\"}}\n# data for the petals is distributed in better clusters than sepals, so this might be better for predictions\n# after some research, i think swarm plots would better be suited for visualizing the variation of length and width per species\n\n\nplt.figure(figsize=(15, 10))\n\n# Petal Length\nplt.subplot(2, 2, 1)\nsns.swarmplot(x='Species', y='PetalLengthCm', data=df)\nplt.title('Swarm Plot of Petal Length by Species')\n\n# Petal Width\nplt.subplot(2, 2, 2)\nsns.swarmplot(x='Species', y='PetalWidthCm', data=df)\nplt.title('Swarm Plot of Petal Width by Species')\n\n# Sepal Length\nplt.subplot(2, 2, 3)\nsns.swarmplot(x='Species', y='SepalLengthCm', data=df)\nplt.title('Swarm Plot of Sepal Length by Species')\n\n# Sepal Width\nplt.subplot(2, 2, 4)\nsns.swarmplot(x='Species', y='SepalWidthCm', data=df)\nplt.title('Swarm Plot of Sepal Width by Species')\n\nplt.tight_layout()\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:50.622044Z\",\"iopub.execute_input\":\"2024-10-13T10:42:50.622752Z\",\"iopub.status.idle\":\"2024-10-13T10:42:50.634720Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:50.622696Z\",\"shell.execute_reply\":\"2024-10-13T10:42:50.633625Z\"}}\ntrain, test = train_test_split(df, test_size = 0.3)\nprint(test.shape)\nprint(train.shape)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:50.636275Z\",\"iopub.execute_input\":\"2024-10-13T10:42:50.636731Z\",\"iopub.status.idle\":\"2024-10-13T10:42:50.650198Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:50.636681Z\",\"shell.execute_reply\":\"2024-10-13T10:42:50.649037Z\"}}\n# we dont need to input id \ntrain_X = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]# taking the training data features\ntrain_y=train.Species# output of our training data\ntest_X= test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] # taking test data features\ntest_y =test.Species   #\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:50.651801Z\",\"iopub.execute_input\":\"2024-10-13T10:42:50.652980Z\",\"iopub.status.idle\":\"2024-10-13T10:42:50.666976Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:50.652927Z\",\"shell.execute_reply\":\"2024-10-13T10:42:50.665915Z\"}}\nfrom sklearn import metrics #for checking the model accuracy\n\nmodel=DecisionTreeClassifier()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,test_y))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:50.668500Z\",\"iopub.execute_input\":\"2024-10-13T10:42:50.669384Z\",\"iopub.status.idle\":\"2024-10-13T10:42:50.685661Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:50.669329Z\",\"shell.execute_reply\":\"2024-10-13T10:42:50.684522Z\"}}\n# 91 percent seems a bit less than ideal. so..\npetal=df[['PetalLengthCm','PetalWidthCm','Species']]\nsepal=df[['SepalLengthCm','SepalWidthCm','Species']]\n\ntrain_p,test_p=train_test_split(petal,test_size=0.3,random_state=0)  #petals\ntrain_x_p=train_p[['PetalWidthCm','PetalLengthCm']]\ntrain_y_p=train_p.Species\ntest_x_p=test_p[['PetalWidthCm','PetalLengthCm']]\ntest_y_p=test_p.Species\n\ntrain_s,test_s=train_test_split(sepal,test_size=0.3,random_state=0)  #Sepal\ntrain_x_s=train_s[['SepalWidthCm','SepalLengthCm']]\ntrain_y_s=train_s.Species\ntest_x_s=test_s[['SepalWidthCm','SepalLengthCm']]\ntest_y_s=test_s.Species\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-13T10:42:50.687202Z\",\"iopub.execute_input\":\"2024-10-13T10:42:50.687557Z\",\"iopub.status.idle\":\"2024-10-13T10:42:50.712959Z\",\"shell.execute_reply.started\":\"2024-10-13T10:42:50.687519Z\",\"shell.execute_reply\":\"2024-10-13T10:42:50.711716Z\"}}\nmodel=DecisionTreeClassifier()\nmodel.fit(train_x_p,train_y_p) \nprediction=model.predict(test_x_p) \nprint('The accuracy of the Decision Tree using Petals is:',metrics.accuracy_score(prediction,test_y_p))\n\nmodel.fit(train_x_s,train_y_s) \nprediction=model.predict(test_x_s) \nprint('The accuracy of the Decision Tree using Sepals is:',metrics.accuracy_score(prediction,test_y_s))\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# # Decision trees\n#     - Non linear model\n#     \n# \n# With decision trees what we wanna do is partition the different regions into +ive and -ive examples in a ***greedy, top down, recursive partitioning.***\n# (Greedy because at each step we want the best partitioning possible)\n# \n# \n# ![image.png](attachment:1088e5f1-659c-4a7c-a891-f88d034d8c0f.png)\n# \n# \n# e.g for the above image:\n# \n# \n# ![image.png](attachment:a8ecf276-f1c1-46eb-a9ea-fed08970f6da.png)\n# \n# \n# \n# \n# # misc :\n# 1.  at positive 90 degrees, you're at the north pole \n# 2.  at negative 90 degrees yoo're at the south pole\n# 3.  0 -> euquator\n# \n# \n# \n# # To summarize:\n# #### Structure:\n#     A Decision Tree consists of nodes, branches, and leaves:\n#     Root Node: The top node that represents the entire dataset.\n#     Internal Nodes: Represent decisions based on feature values.\n#     Leaf Nodes: Represent the final output or prediction.\n#     \n# #### Splitting:\n#     The process of dividing the dataset into subsets based on feature values. Each split aims to create homogeneous subsets regarding the target variable.\n#     The algorithm selects the best feature and threshold to split the data, maximizing some measure of impurity reduction.\n# \n# ####  Impurity Measures:\n#     Common measures to determine the quality of a split:\n#         1. Gini Impurity: Measures the impurity of a node; lower values indicate better purity.\n#         2. Entropy: Used in Information Gain; measures the uncertainty in a dataset.\n#         3. Mean Squared Error (MSE): Used for regression tasks, assessing the variance of the target variable in the split.\n# \n# #### Recursive Partitioning:\n# \n#     The tree is built using a recursive process:\n#     Start with the root node and repeatedly split the dataset until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf, or no further impurity improvement).\n# \n# #### Stopping Criteria:\n#     Criteria to halt the growth of the tree to prevent overfitting:\n#     1. Maximum depth of the tree.\n#     2. Minimum samples required to split a node.\n#     3. Minimum impurity decrease for a split.\n# \n# #### Overfitting:\n#     Decision Trees can easily overfit the training data if they are too deep. Overfitting occurs when the model captures noise rather than the underlying pattern.\n#     Techniques such as pruning, setting a maximum depth, or limiting the number of leaf nodes can mitigate this.\n# \n# #### Pruning:\n#     A technique to reduce the size of the tree by removing sections that provide little power in predicting target outcomes, helping to improve generalization.\n#     Can be done:\n#     Pre-pruning: Stop the growth of the tree early.\n#     Post-pruning: Allow the full tree to grow and then trim it.\n# \n# #### Interpretability:\n#     One of the biggest advantages of Decision Trees is their interpretability. You can easily visualize the decision-making process, which can be presented as a flowchart.\n#     \n# #### Handling Missing Values:\n#     Decision Trees can handle missing values naturally, as they can assign data points to branches based on available features.\n# \n# ####  Limitations:\n#     Decision Trees can be sensitive to noisy data and small changes in the dataset can lead to different structures.\n#     They tend to favor features with more levels or categories, which can lead to bias.\n# \n\n# %% [markdown]\n# \n\n# %% [markdown]\n# ","metadata":{"_uuid":"56ee9e1f-acaf-4684-be86-04ee926c3316","_cell_guid":"7d8be10f-08fd-4b43-a5d5-e8ff2ea2be15","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}